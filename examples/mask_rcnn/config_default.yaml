# ---------------------------------------------------------------------------- #
# Attribute:
# (TRAIN, DATA_LOADER, TEST, MODEL, RETINANET, SOLVER, FAST_RCNN, RPN, FPN, 
#  MRCNN, KRCNN, RFCN, RESNETS, GROUP_NORM)
# (NUM_GPUS, DEDUP_BOXES, BBOX_XFORM_CLIP, PIXEL_MEANS, RNG_SEED, EPS, ROOT_DIR
#  OUTPUT_DIR, MATLAB, VIS, VIS_TH, EXPECTED_RESULTS, EXPECTED_RESULTS_RTOL,
#  EXPECTED_RESULTS_ATOL, EXPECTED_RESULTS_EMAIL, DATA_DIR, POOLING_MODE, 
#  POOLING_SIZE, CROP_RESIZE_WITH_MAX_POOL, CUDA, DEBUG, PYTORCH_VERSION_LESS_THAN_040)
# ---------------------------------------------------------------------------- #


# ---------------------------------------------------------------------------- #
# Training options
# ---------------------------------------------------------------------------- #
TRAIN:

    # Datasets to train on
    # Available dataset list: datasets.dataset_catalog.DATASETS.keys()
    # If multiple datasets are listed, the model is trained on their union
    DATASETS: [] #()

    # Scales to use during training
    # Each scale is the pixel size of an image's shortest side
    # If multiple scales are listed, then one is selected uniformly at random for
    # each training image (i.e., scale jitter data augmentation)
    SCALES: [600, ] #(600, )

    # Max pixel size of the longest side of a scaled input image
    MAX_SIZE: 1000

    # Images *per GPU* in the training minibatch
    # Total images per minibatch = TRAIN.IMS_PER_BATCH * NUM_GPUS
    IMS_PER_BATCH: 2

    # RoI minibatch size *per image* (number of regions of interest [ROIs])
    # Total number of RoIs per training minibatch =
    #   TRAIN.BATCH_SIZE_PER_IM * TRAIN.IMS_PER_BATCH * NUM_GPUS
    # E.g., a common configuration is: 512 * 2 * 8 = 8192
    BATCH_SIZE_PER_IM: 64

    # Fraction of minibatch that is labeled foreground (i.e. class > 0)
    FG_FRACTION: 0.25

    # Overlap threshold for a ROI to be considered foreground (if >= FG_THRESH)
    FG_THRESH: 0.5

    # Overlap threshold for a ROI to be considered background (class = 0 if
    # overlap in [LO, HI))
    BG_THRESH_HI: 0.5
    BG_THRESH_LO: 0.0

    # Use horizontally-flipped images during training?
    USE_FLIPPED: True

    # Overlap required between a ROI and ground-truth box in order for that ROI to
    # be used as a bounding-box regression training example
    BBOX_THRESH: 0.5

    # Train using these proposals
    # During training, all proposals specified in the file are used (no limit is
    # applied)
    # Proposal files must be in correspondence with the datasets listed in
    # TRAIN.DATASETS
    PROPOSAL_FILES: [] #()

    # Snapshot (model checkpoint) period
    # Divide by NUM_GPUS to determine actual period (e.g., 20000/8 => 2500 iters)
    # to allow for linear training schedule scaling
    SNAPSHOT_ITERS: 20000

    # Normalize the targets (subtract empirical mean, divide by empirical stddev)
    BBOX_NORMALIZE_TARGETS: True
    # Deprecated (inside weights)
    BBOX_INSIDE_WEIGHTS: [1.0, 1.0, 1.0, 1.0] #(1.0, 1.0, 1.0, 1.0)
    # Normalize the targets using "precomputed" (or made up) means and stdevs
    # (BBOX_NORMALIZE_TARGETS must also be True) (legacy)
    BBOX_NORMALIZE_TARGETS_PRECOMPUTED: False
    BBOX_NORMALIZE_MEANS: [0.0, 0.0, 0.0, 0.0] #(0.0, 0.0, 0.0, 0.0)
    BBOX_NORMALIZE_STDS: [0.1, 0.1, 0.2, 0.2] #(0.1, 0.1, 0.2, 0.2)

    # Make minibatches from images that have similar aspect ratios (i.e. both
    # tall and thin or both short and wide)
    # This feature is critical for saving memory (and makes training slightly
    # faster)
    ASPECT_GROUPING: True

    # Crop images that have too small or too large aspect ratio
    ASPECT_CROPPING: False
    ASPECT_HI: 2
    ASPECT_LO: 0.5

    # ---------------------------------------------------------------------------- #
    # RPN training options
    # ---------------------------------------------------------------------------- #

    # Minimum overlap required between an anchor and ground-truth box for the
    # (anchor, gt box) pair to be a positive example (IOU >= thresh ==> positive RPN
    # example)
    RPN_POSITIVE_OVERLAP: 0.7

    # Maximum overlap allowed between an anchor and ground-truth box for the
    # (anchor, gt box) pair to be a negative examples (IOU < thresh ==> negative RPN
    # example)
    RPN_NEGATIVE_OVERLAP: 0.3

    # Target fraction of foreground (positive) examples per RPN minibatch
    RPN_FG_FRACTION: 0.5

    # Total number of RPN examples per image
    RPN_BATCH_SIZE_PER_IM: 256

    # NMS threshold used on RPN proposals (used during end-to-end training with RPN)
    RPN_NMS_THRESH: 0.7

    # Number of top scoring RPN proposals to keep before applying NMS (per image)
    # When FPN is used, this is *per FPN level* (not total)
    RPN_PRE_NMS_TOP_N: 12000

    # Number of top scoring RPN proposals to keep after applying NMS (per image)
    # This is the total number of RPN proposals produced (for both FPN and non-FPN
    # cases)
    RPN_POST_NMS_TOP_N: 2000

    # Remove RPN anchors that go outside the image by RPN_STRADDLE_THRESH pixels
    # Set to -1 or a large value, e.g. 100000, to disable pruning anchors
    RPN_STRADDLE_THRESH: 0

    # Proposal height and width both need to be greater than RPN_MIN_SIZE
    # (at orig image scale; not scale used during training or inference)
    RPN_MIN_SIZE: 0

    # Filter proposals that are inside of crowd regions by CROWD_FILTER_THRESH
    # "Inside" is measured as: proposal-with-crowd intersection area divided by
    # proposal area
    CROWD_FILTER_THRESH: 0.7

    # Ignore ground-truth objects with area < this threshold
    GT_MIN_AREA: -1

    # Freeze the backbone architecture during training if set to True
    FREEZE_CONV_BODY: False


# ---------------------------------------------------------------------------- #
# Data loader options
# ---------------------------------------------------------------------------- #
DATA_LOADER:

    # Number of Python threads to use for the data loader (warning: using too many
    # threads can cause GIL-based interference with Python Ops leading to *slower*
    # training; 4 seems to be the sweet spot in our experience)
    NUM_THREADS: 4


# ---------------------------------------------------------------------------- #
# Inference ('test') options
# ---------------------------------------------------------------------------- #
TEST:

    # Datasets to test on
    # Available dataset list: datasets.dataset_catalog.DATASETS.keys()
    # If multiple datasets are listed, testing is performed on each one sequentially
    DATASETS: [] #()

    # Scale to use during testing (can NOT list multiple scales)
    # The scale is the pixel size of an image's shortest side
    SCALE: 600

    # Max pixel size of the longest side of a scaled input image
    MAX_SIZE: 1000

    # Overlap threshold used for non-maximum suppression (suppress boxes with
    # IoU >= this threshold)
    NMS: 0.3

    # Apply Fast R-CNN style bounding-box regression if True
    BBOX_REG: True

    # Test using these proposal files (must correspond with TEST.DATASETS)
    PROPOSAL_FILES: [] #()

    # Limit on the number of proposals per image used during inference
    PROPOSAL_LIMIT: 2000

    # NMS threshold used on RPN proposals
    RPN_NMS_THRESH: 0.7

    # Number of top scoring RPN proposals to keep before applying NMS
    # When FPN is used, this is *per FPN level* (not total)
    RPN_PRE_NMS_TOP_N: 12000

    # Number of top scoring RPN proposals to keep after applying NMS
    # This is the total number of RPN proposals produced (for both FPN and non-FPN
    # cases)
    RPN_POST_NMS_TOP_N: 2000

    # Proposal height and width both need to be greater than RPN_MIN_SIZE
    # (at orig image scale; not scale used during training or inference)
    RPN_MIN_SIZE: 0

    # Maximum number of detections to return per image (100 is based on the limit
    # established for the COCO dataset)
    DETECTIONS_PER_IM: 100

    # Minimum score threshold (assuming scores in a [0, 1] range); a value chosen to
    # balance obtaining high recall with not having too many low precision
    # detections that will slow down inference post processing steps (like NMS)
    SCORE_THRESH: 0.05

    # Save detection results files if True
    # If false, results files are cleaned up (they can be large) after local
    # evaluation
    COMPETITION_MODE: True

    # Evaluate detections with the COCO json dataset eval code even if it's not the
    # evaluation code for the dataset (e.g. evaluate PASCAL VOC results using the
    # COCO API to get COCO style AP on PASCAL VOC)
    FORCE_JSON_DATASET_EVAL: False

    # [Inferred value; do not set directly in a config]
    # Indicates if precomputed proposals are used at test time
    # Not set for 1-stage models and 2-stage models with RPN subnetwork enabled
    PRECOMPUTED_PROPOSALS: True


    # ---------------------------------------------------------------------------- #
    # Test-time augmentations for bounding box detection
    # See configs/test_time_aug/e2e_mask_rcnn_R-50-FPN_2x.yaml for an example
    # ---------------------------------------------------------------------------- #
    BBOX_AUG:

        # Enable test-time augmentation for bounding box detection if True
        ENABLED: False

        # Heuristic used to combine predicted box scores
        #   Valid options: ('ID', 'AVG', 'UNION')
        SCORE_HEUR: 'UNION'

        # Heuristic used to combine predicted box coordinates
        #   Valid options: ('ID', 'AVG', 'UNION')
        COORD_HEUR: 'UNION'

        # Horizontal flip at the original scale (id transform)
        H_FLIP: False

        # Each scale is the pixel size of an image's shortest side
        SCALES: [] #()

        # Max pixel size of the longer side
        MAX_SIZE: 4000

        # Horizontal flip at each scale
        SCALE_H_FLIP: False

        # Apply scaling based on object size
        SCALE_SIZE_DEP: False
        AREA_TH_LO: 2500 #50**2
        AREA_TH_HI: 32400 #180**2

        # Each aspect ratio is relative to image width
        ASPECT_RATIOS: [] #()

        # Horizontal flip at each aspect ratio
        ASPECT_RATIO_H_FLIP: False

    # ---------------------------------------------------------------------------- #
    # Test-time augmentations for mask detection
    # See configs/test_time_aug/e2e_mask_rcnn_R-50-FPN_2x.yaml for an example
    # ---------------------------------------------------------------------------- #
    MASK_AUG:

        # Enable test-time augmentation for instance mask detection if True
        ENABLED: False

        # Heuristic used to combine mask predictions
        # SOFT prefix indicates that the computation is performed on soft masks
        #   Valid options: ('SOFT_AVG', 'SOFT_MAX', 'LOGIT_AVG')
        HEUR: 'SOFT_AVG'

        # Horizontal flip at the original scale (id transform)
        H_FLIP: False

        # Each scale is the pixel size of an image's shortest side
        SCALES: [] #()

        # Max pixel size of the longer side
        MAX_SIZE: 4000

        # Horizontal flip at each scale
        SCALE_H_FLIP: False

        # Apply scaling based on object size
        SCALE_SIZE_DEP: False
        AREA_TH: 32400 #180**2

        # Each aspect ratio is relative to image width
        ASPECT_RATIOS: [] #()

        # Horizontal flip at each aspect ratio
        ASPECT_RATIO_H_FLIP: False

    # ---------------------------------------------------------------------------- #
    # Test-augmentations for keypoints detection
    # configs/test_time_aug/keypoint_rcnn_R-50-FPN_1x.yaml
    # ---------------------------------------------------------------------------- #
    KPS_AUG:

        # Enable test-time augmentation for keypoint detection if True
        ENABLED: False

        # Heuristic used to combine keypoint predictions
        #   Valid options: ('HM_AVG', 'HM_MAX')
        HEUR: 'HM_AVG'

        # Horizontal flip at the original scale (id transform)
        H_FLIP: False

        # Each scale is the pixel size of an image's shortest side
        SCALES: [] #()

        # Max pixel size of the longer side
        MAX_SIZE: 4000

        # Horizontal flip at each scale
        SCALE_H_FLIP: False

        # Apply scaling based on object size
        SCALE_SIZE_DEP: False
        AREA_TH: 32400 #180**2

        # Eeach aspect ratio is realtive to image width
        ASPECT_RATIOS: [] #()

        # Horizontal flip at each aspect ratio
        ASPECT_RATIO_H_FLIP: False

    # ---------------------------------------------------------------------------- #
    # Soft NMS
    # ---------------------------------------------------------------------------- #
    SOFT_NMS:

        # Use soft NMS instead of standard NMS if set to True
        ENABLED: False
        # See soft NMS paper for definition of these options
        METHOD: 'linear'
        SIGMA: 0.5
        # For the soft NMS overlap threshold, we simply use TEST.NMS

    # ---------------------------------------------------------------------------- #
    # Bounding box voting (from the Multi-Region CNN paper)
    # ---------------------------------------------------------------------------- #
    BBOX_VOTE:

        # Use box voting if set to True
        ENABLED: False

        # We use TEST.NMS threshold for the NMS step. VOTE_TH overlap threshold
        # is used to select voting boxes (IoU >= VOTE_TH) for each box that survives NMS
        VOTE_TH: 0.8

        # The method used to combine scores when doing bounding box voting
        # Valid options include ('ID', 'AVG', 'IOU_AVG', 'GENERALIZED_AVG', 'QUASI_SUM')
        SCORING_METHOD: 'ID'

        # Hyperparameter used by the scoring method (it has different meanings for
        # different methods)
        SCORING_METHOD_BETA: 1.0


# ---------------------------------------------------------------------------- #
# Model options
# ---------------------------------------------------------------------------- #
MODEL:

    # The type of model to use
    # The string must match a function in the modeling.model_builder module
    # (e.g., 'generalized_rcnn', 'mask_rcnn', ...)
    TYPE: ''

    # The backbone conv body to use
    CONV_BODY: ''

    # Number of classes in the dataset; must be set
    # E.g., 81 for COCO (80 foreground + 1 background)
    NUM_CLASSES: -1

    # Use a class agnostic bounding box regressor instead of the default per-class
    # regressor
    CLS_AGNOSTIC_BBOX_REG: False

    # Default weights on (dx, dy, dw, dh) for normalizing bbox regression targets
    # These are empirically chosen to approximately lead to unit variance targets
    #
    # In older versions, the weights were set such that the regression deltas
    # would have unit standard deviation on the training dataset. Presently, rather
    # than computing these statistics exactly, we use a fixed set of weights
    # (10., 10., 5., 5.) by default. These are approximately the weights one would
    # get from COCO using the previous unit stdev heuristic.
    BBOX_REG_WEIGHTS: [10., 10., 5., 5.] #(10., 10., 5., 5.)

    # The meaning of FASTER_RCNN depends on the context (training vs. inference):
    # 1) During training, FASTER_RCNN = True means that end-to-end training will be
    #    used to jointly train the RPN subnetwork and the Fast R-CNN subnetwork
    #    (Faster R-CNN = RPN + Fast R-CNN).
    # 2) During inference, FASTER_RCNN = True means that the model's RPN subnetwork
    #    will be used to generate proposals rather than relying on precomputed
    #    proposals. Note that FASTER_RCNN = True can be used at inference time even
    #    if the Faster R-CNN model was trained with stagewise training (which
    #    consists of alternating between RPN and Fast R-CNN training in a way that
    #    finally leads to a single network).
    FASTER_RCNN: False

    # Indicates the model makes instance mask predictions (as in Mask R-CNN)
    MASK_ON: False

    # Indicates the model makes keypoint predictions (as in Mask R-CNN for
    # keypoints)
    KEYPOINTS_ON: False

    # Indicates the model's computation terminates with the production of RPN
    # proposals (i.e., it outputs proposals ONLY, no actual object detections)
    RPN_ONLY: False

    # [Inferred value; do not set directly in a config]
    # Indicate whether the res5 stage weights and training forward computation
    # are shared from box head or not.
    SHARE_RES5: False

    # Whether to load imagenet pretrained weights
    # If True, path to the weight file must be specified.
    # See: RESNETS.IMAGENET_PRETRAINED_WEIGHTS
    LOAD_IMAGENET_PRETRAINED_WEIGHTS: True

    # ---------------------------------------------------------------------------- #
    # Unsupervise Pose
    # ---------------------------------------------------------------------------- #

    UNSUPERVISED_POSE: False


# ---------------------------------------------------------------------------- #
# RetinaNet options (Not Implement)
# ---------------------------------------------------------------------------- #
RETINANET:
    RETINANET_ON: False

# ---------------------------------------------------------------------------- #
# Solver options
# Note: all solver options are used exactly as specified; the implication is
# that if you switch from training on 1 GPU to N GPUs, you MUST adjust the
# solver configuration accordingly. We suggest using gradual warmup and the
# linear learning rate scaling rule as described in
# "Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour" Goyal et al.
# https://arxiv.org/abs/1706.02677
# ---------------------------------------------------------------------------- #
SOLVER:

    # e.g 'SGD', 'Adam'
    TYPE: 'SGD'

    # Base learning rate for the specified schedule
    BASE_LR: 0.001

    # Schedule type (see functions in utils.lr_policy for options)
    # E.g., 'step', 'steps_with_decay', ...
    LR_POLICY: 'step'

    # Some LR Policies (by example):
    # 'step'
    #   lr = SOLVER.BASE_LR * SOLVER.GAMMA ** (cur_iter // SOLVER.STEP_SIZE)
    # 'steps_with_decay'
    #   SOLVER.STEPS = [0, 60000, 80000]
    #   SOLVER.GAMMA = 0.1
    #   lr = SOLVER.BASE_LR * SOLVER.GAMMA ** current_step
    #   iters [0, 59999] are in current_step = 0, iters [60000, 79999] are in
    #   current_step = 1, and so on
    # 'steps_with_lrs'
    #   SOLVER.STEPS = [0, 60000, 80000]
    #   SOLVER.LRS = [0.02, 0.002, 0.0002]
    #   lr = LRS[current_step]

    # Hyperparameter used by the specified policy
    # For 'step', the current LR is multiplied by SOLVER.GAMMA at each step
    GAMMA: 0.1

    # Uniform step size for 'steps' policy
    STEP_SIZE: 30000

    # Non-uniform step iterations for 'steps_with_decay' or 'steps_with_lrs'
    # policies
    STEPS: []

    # Learning rates to use with 'steps_with_lrs' policy
    LRS: []

    # Maximum number of SGD iterations
    MAX_ITER: 40000

    # Momentum to use with SGD
    MOMENTUM: 0.9

    # L2 regularization hyperparameter
    WEIGHT_DECAY: 0.0005
    # L2 regularization hyperparameter for GroupNorm's parameters
    WEIGHT_DECAY_GN: 0.0

    # Whether to double the learning rate for bias
    BIAS_DOUBLE_LR: True

    # Whether to have weight decay on bias as well
    BIAS_WEIGHT_DECAY: False

    # Warm up to SOLVER.BASE_LR over this number of SGD iterations
    WARM_UP_ITERS: 500

    # Start the warm up from SOLVER.BASE_LR * SOLVER.WARM_UP_FACTOR
    WARM_UP_FACTOR: 0.33333333333333 # 1.0 / 3.0

    # WARM_UP_METHOD can be either 'constant' or 'linear' (i.e., gradual)
    WARM_UP_METHOD: 'linear'

    # Scale the momentum update history by new_lr / old_lr when updating the
    # learning rate (this is correct given MomentumSGDUpdateOp)
    SCALE_MOMENTUM: True
    # Only apply the correction if the relative LR change exceeds this threshold
    # (prevents ever change in linear warm up from scaling the momentum by a tiny
    # amount; momentum scaling is only important if the LR change is large)
    SCALE_MOMENTUM_THRESHOLD: 1.1

    # Suppress logging of changes to LR unless the relative change exceeds this
    # threshold (prevents linear warm up from spamming the training log)
    LOG_LR_CHANGE_THRESHOLD: 1.1


# ---------------------------------------------------------------------------- #
# Fast R-CNN options
# ---------------------------------------------------------------------------- #
FAST_RCNN:

    # The type of RoI head to use for bounding box classification and regression
    # The string must match a function this is imported in modeling.model_builder
    # (e.g., 'head_builder.add_roi_2mlp_head' to specify a two hidden layer MLP)
    ROI_BOX_HEAD: ''

    # Hidden layer dimension when using an MLP for the RoI box head
    MLP_HEAD_DIM: 1024

    # Hidden Conv layer dimension when using Convs for the RoI box head
    CONV_HEAD_DIM: 256
    # Number of stacked Conv layers in the RoI box head
    NUM_STACKED_CONVS: 4

    # RoI transformation function (e.g., RoIPool or RoIAlign)
    # (RoIPoolF is the same as RoIPool; ignore the trailing 'F')
    ROI_XFORM_METHOD: 'RoIPoolF'

    # Number of grid sampling points in RoIAlign (usually use 2)
    # Only applies to RoIAlign
    ROI_XFORM_SAMPLING_RATIO: 0

    # RoI transform output resolution
    # Note: some models may have constraints on what they can use, e.g. they use
    # pretrained FC layers like in VGG16, and will ignore this option
    ROI_XFORM_RESOLUTION: 14


# ---------------------------------------------------------------------------- #
# RPN options
# ---------------------------------------------------------------------------- #
RPN:

    # [Infered value; do not set directly in a config]
    # Indicates that the model contains an RPN subnetwork
    RPN_ON: False

    # `True` for Detectron implementation. `False` for jwyang's implementation.
    OUT_DIM_AS_IN_DIM: True

    # Output dim of conv2d. Ignored if `OUT_DIM_AS_IN_DIM` is True.
    # 512 is the fixed value in jwyang's implementation.
    OUT_DIM: 512

    # 'sigmoid' or 'softmax'. Detectron use 'sigmoid'. jwyang use 'softmax'
    # This will affect the conv2d output dim for classifying the bg/fg rois
    CLS_ACTIVATION: 'sigmoid'

    # RPN anchor sizes given in absolute pixels w.r.t. the scaled network input
    # Note: these options are *not* used by FPN RPN; see FPN.RPN* options
    SIZES: [64, 128, 256, 512] #(64, 128, 256, 512)

    # Stride of the feature map that RPN is attached
    STRIDE: 16

    # RPN anchor aspect ratios
    ASPECT_RATIOS: [0.5, 1, 2] #(0.5, 1, 2)


# ---------------------------------------------------------------------------- #
# FPN options
# ---------------------------------------------------------------------------- #

FPN:

    # FPN is enabled if True
    FPN_ON: False

    # Channel dimension of the FPN feature levels
    DIM: 256

    # Initialize the lateral connections to output zero if True
    ZERO_INIT_LATERAL: False

    # Stride of the coarsest FPN level
    # This is needed so the input can be padded properly
    COARSEST_STRIDE: 32

    #
    # FPN may be used for just RPN, just object detection, or both
    #

    # Use FPN for RoI transform for object detection if True
    MULTILEVEL_ROIS: False
    # Hyperparameters for the RoI-to-FPN level mapping heuristic
    ROI_CANONICAL_SCALE: 224      # s0
    ROI_CANONICAL_LEVEL: 4      # k0: where s0 maps to
    # Coarsest level of the FPN pyramid
    ROI_MAX_LEVEL: 5
    # Finest level of the FPN pyramid
    ROI_MIN_LEVEL: 2

    # Use FPN for RPN if True
    MULTILEVEL_RPN: False
    # Coarsest level of the FPN pyramid
    RPN_MAX_LEVEL: 6
    # Finest level of the FPN pyramid
    RPN_MIN_LEVEL: 2
    # FPN RPN anchor aspect ratios
    RPN_ASPECT_RATIOS: [0.5, 1, 2] #(0.5, 1, 2)
    # RPN anchors start at this size on RPN_MIN_LEVEL
    # The anchor size doubled each level after that
    # With a default of 32 and levels 2 to 6, we get anchor sizes of 32 to 512
    RPN_ANCHOR_START_SIZE: 32
    # [Infered Value] Scale for RPN_POST_NMS_TOP_N.
    # Automatically infered in training, fixed to 1 in testing.
    RPN_COLLECT_SCALE: 1
    # Use extra FPN levels, as done in the RetinaNet paper
    EXTRA_CONV_LEVELS: False
    # Use GroupNorm in the FPN-specific layers (lateral, etc.)
    USE_GN: False


# ---------------------------------------------------------------------------- #
# Mask R-CNN options ("MRCNN" means Mask R-CNN)
# ---------------------------------------------------------------------------- #
MRCNN:

    # The type of RoI head to use for instance mask prediction
    # The string must match a function this is imported in modeling.model_builder
    # (e.g., 'mask_rcnn_heads.ResNet_mask_rcnn_fcn_head_v1up4convs')
    ROI_MASK_HEAD: ''

    # Resolution of mask predictions
    RESOLUTION: 14

    # RoI transformation function and associated options
    ROI_XFORM_METHOD: 'RoIAlign'

    # RoI transformation function (e.g., RoIPool or RoIAlign)
    ROI_XFORM_RESOLUTION: 7

    # Number of grid sampling points in RoIAlign (usually use 2)
    # Only applies to RoIAlign
    ROI_XFORM_SAMPLING_RATIO: 0

    # Number of channels in the mask head
    DIM_REDUCED: 256

    # Use dilated convolution in the mask head
    DILATION: 2

    # Upsample the predicted masks by this factor
    UPSAMPLE_RATIO: 1

    # Use a fully-connected layer to predict the final masks instead of a conv layer
    USE_FC_OUTPUT: False

    # Weight initialization method for the mask head and mask output layers. ['GaussianFill', 'MSRAFill']
    CONV_INIT: 'GaussianFill'

    # Use class specific mask predictions if True (otherwise use class agnostic mask
    # predictions)
    CLS_SPECIFIC_MASK: True

    # Multi-task loss weight for masks
    WEIGHT_LOSS_MASK: 1.0

    # Binarization threshold for converting soft masks to hard masks
    THRESH_BINARIZE: 0.5

    MEMORY_EFFICIENT_LOSS: True  # TODO


# ---------------------------------------------------------------------------- #
# Keyoint Mask R-CNN options ("KRCNN" = Mask R-CNN with Keypoint support)
# ---------------------------------------------------------------------------- #
KRCNN:

    # The type of RoI head to use for instance keypoint prediction
    # The string must match a function this is imported in modeling.model_builder
    # (e.g., 'keypoint_rcnn_heads.add_roi_pose_head_v1convX')
    ROI_KEYPOINTS_HEAD: ''

    # Output size (and size loss is computed on), e.g., 56x56
    HEATMAP_SIZE: -1

    # Use bilinear interpolation to upsample the final heatmap by this factor
    UP_SCALE: -1

    # Apply a ConvTranspose layer to the hidden representation computed by the
    # keypoint head prior to predicting the per-keypoint heatmaps
    USE_DECONV: False
    # Channel dimension of the hidden representation produced by the ConvTranspose
    DECONV_DIM: 256

    # Use a ConvTranspose layer to predict the per-keypoint heatmaps
    USE_DECONV_OUTPUT: False

    # Use dilation in the keypoint head
    DILATION: 1

    # Size of the kernels to use in all ConvTranspose operations
    DECONV_KERNEL: 4

    # Number of keypoints in the dataset (e.g., 17 for COCO)
    NUM_KEYPOINTS: -1

    # Number of stacked Conv layers in keypoint head
    NUM_STACKED_CONVS: 8

    # Dimension of the hidden representation output by the keypoint head
    CONV_HEAD_DIM: 256

    # Conv kernel size used in the keypoint head
    CONV_HEAD_KERNEL: 3
    # Conv kernel weight filling function
    CONV_INIT: 'GaussianFill'

    # Use NMS based on OKS if True
    NMS_OKS: False

    # Source of keypoint confidence
    #   Valid options: ('bbox', 'logit', 'prob')
    KEYPOINT_CONFIDENCE: 'bbox'

    # Standard ROI XFORM options (see FAST_RCNN or MRCNN options)
    ROI_XFORM_METHOD: 'RoIAlign'
    ROI_XFORM_RESOLUTION: 7
    ROI_XFORM_SAMPLING_RATIO: 0

    # Minimum number of labeled keypoints that must exist in a minibatch (otherwise
    # the minibatch is discarded)
    MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH: 20

    # When infering the keypoint locations from the heatmap, don't scale the heatmap
    # below this minimum size
    INFERENCE_MIN_SIZE: 0

    # Multi-task loss weight to use for keypoints
    # Recommended values:
    #   - use 1.0 if KRCNN.NORMALIZE_BY_VISIBLE_KEYPOINTS is True
    #   - use 4.0 if KRCNN.NORMALIZE_BY_VISIBLE_KEYPOINTS is False
    LOSS_WEIGHT: 1.0

    # Normalize by the total number of visible keypoints in the minibatch if True.
    # Otherwise, normalize by the total number of keypoints that could ever exist
    # in the minibatch. See comments in modeling.model_builder.add_keypoint_losses
    # for detailed discussion.
    NORMALIZE_BY_VISIBLE_KEYPOINTS: True


# ---------------------------------------------------------------------------- #
# R-FCN options
# ---------------------------------------------------------------------------- #
RFCN:

    # Position-sensitive RoI pooling output grid size (height and width)
    PS_GRID_SIZE: 3


# ---------------------------------------------------------------------------- #
# ResNets options ("ResNets" = ResNet and ResNeXt)
# ---------------------------------------------------------------------------- #
RESNETS:

    # Number of groups to use; 1 ==> ResNet; > 1 ==> ResNeXt
    NUM_GROUPS: 1

    # Baseline width of each group
    WIDTH_PER_GROUP: 64

    # Place the stride 2 conv on the 1x1 filter
    # Use True only for the original MSRA ResNet; use False for C2 and Torch models
    STRIDE_1X1: True

    # Residual transformation function
    TRANS_FUNC: 'bottleneck_transformation'
    # ResNet's stem function (conv1 and pool1)
    STEM_FUNC: 'basic_bn_stem'
    # ResNet's shortcut function
    SHORTCUT_FUNC: 'basic_bn_shortcut'

    # Apply dilation in stage "res5"
    RES5_DILATION: 1

    # Freeze model weights before and including which block.
    # Choices: [0, 2, 3, 4, 5]. O means not fixed. First conv and bn are defaults to
    # be fixed.
    FREEZE_AT: 2

    # Path to pretrained resnet weights on ImageNet.
    # If start with '/', then it is treated as a absolute path.
    # Otherwise, treat as a relative path to ROOT_DIR
    IMAGENET_PRETRAINED_WEIGHTS: ''

    # Use GroupNorm instead of BatchNorm
    USE_GN: False


# ---------------------------------------------------------------------------- #
# GroupNorm options
# ---------------------------------------------------------------------------- #
GROUP_NORM:
    # Number of dimensions per group in GroupNorm (-1 if using NUM_GROUPS)
    GROUP_NORM.DIM_PER_GP: -1
    # Number of groups in GroupNorm (-1 if using DIM_PER_GP)
    GROUP_NORM.NUM_GROUPS: 32
    # GroupNorm's small constant in the denominator
    GROUP_NORM.EPSILON: 1e-5


# ---------------------------------------------------------------------------- #
# MISC options
# ---------------------------------------------------------------------------- #

# Number of GPUs to use (applies to both training and testing)
NUM_GPUS: 1

# The mapping from image coordinates to feature map coordinates might cause
# some boxes that are distinct in image space to become identical in feature
# coordinates. If DEDUP_BOXES > 0, then DEDUP_BOXES is used as the scale factor
# for identifying duplicate boxes.
# 1/16 is correct for {Alex,Caffe}Net, VGG_CNN_M_1024, and VGG16
DEDUP_BOXES: 0.0625 #1. / 16.

# Clip bounding box transformation predictions to prevent np.exp from
# overflowing
# Heuristic choice based on that would scale a 16 pixel anchor up to 1000 pixels
BBOX_XFORM_CLIP: 4.135166556742356 #np.log(1000. / 16.)

# Pixel mean values (BGR order) as a (1, 1, 3) array
# We use the same pixel mean for all networks even though it's not exactly what
# they were trained with
# "Fun" fact: the history of where these values comes from is lost (From Detectron lol)
PIXEL_MEANS: [102.9801, 115.9465, 122.7717] #np.array([[[102.9801, 115.9465, 122.7717]]])

# For reproducibility
RNG_SEED: 3

# A small number that's used many times
EPS: 1e-14

# Root directory of project
ROOT_DIR: '/home/dalong/data/toolbox_pytorch/examples/mask_rcnn' #osp.abspath(osp.join(osp.dirname(__file__), '..', '..'))

# Output basedir
OUTPUT_DIR: 'Outputs'

# Name (or path to) the matlab executable
MATLAB: 'matlab'

# Dump detection visualizations
VIS: False

# Score threshold for visualization
VIS_TH: 0.9

# Expected results should take the form of a list of expectations, each
# specified by four elements (dataset, task, metric, expected value). For
# example: [['coco_2014_minival', 'box_proposal', 'AR@1000', 0.387]]
EXPECTED_RESULTS: []
# Absolute and relative tolerance to use when comparing to EXPECTED_RESULTS
EXPECTED_RESULTS_RTOL: 0.1
EXPECTED_RESULTS_ATOL: 0.005
# Set to send email in case of an EXPECTED_RESULTS failure
EXPECTED_RESULTS_EMAIL: ''

# ------------------------------
# Data directory
DATA_DIR: '/home/dalong/data/toolbox_pytorch/examples/mask_rcnn/data' #osp.abspath(osp.join(ROOT_DIR, 'data'))

# [Deprecate]
POOLING_MODE: 'crop'

# [Deprecate] Size of the pooled region after RoI pooling
POOLING_SIZE: 7

CROP_RESIZE_WITH_MAX_POOL: True

# [Infered value]
CUDA: False

DEBUG: False

# [Infered value]
PYTORCH_VERSION_LESS_THAN_040: False


    