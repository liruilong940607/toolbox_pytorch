NAME: 'Mask_RCNN'

MYDEBUG: True

MYDATASET:
    TRAIN_DIR: '/home/dalong/data/coco2017/train2017'
    VAL_DIR: '/home/dalong/data/coco2017/val2017'
    TRAIN_ANNOFILE: '/home/dalong/data/coco2017/annotations/instances_train2017.json'
    VAL_ANNOFILE: '/home/dalong/data/coco2017/annotations/instances_train2017.json'

    MEAN: [102.9801, 115.9465, 122.7717]     # BGR

INIT: ~

MYSOLVER:
    WORKERS: 2
    IMS_PER_BATCH: 2 # same as CONFIG.TRAIN.IMS_PER_BATCH
    GPU_IDS: [0, ]     # real BATCHSIZE is : IMG_PER_GPU * len(GPU_IDS) 
    # LR: 0.0001
    # LR_SCHEDULE: 'standard'
    # OPTIMIZER: 'ADAM'     # ADAM | SGD
    # MOMENTUM: 0.9
    # WEIGHTDECAY: 0.0004

LOGS:
    PRINT_FREQ: 10
    SNAPSHOT_FREQ: 1000
    SNAPSHOT_MAXFILES: NOTIMPLEMENT
    SNAPSHOT_DIR: 'snapshot'
    LOG_FREQ: 100
    LOG_DIR: 'logs'
    LOG_SHUTIL_ON: true
    LOG_SHUTIL_IGNORELIST: ['.git', 'snapshot', 'logs']
    LOG_SHUTIL_IGNOREFILE: '.gitignore'
    
# ---------------------------------------------------------------------------- #
# e2e_mask_rcnn_R-50-FPN_1x.yaml
# ---------------------------------------------------------------------------- #

MODEL:
    TYPE: generalized_rcnn
    CONV_BODY: FPN.fpn_ResNet50_conv5_body
    FASTER_RCNN: True
    MASK_ON: True
RESNETS:
    IMAGENET_PRETRAINED_WEIGHTS: 'data/pretrained_model/resnet50_caffe.pth'
NUM_GPUS: 8
SOLVER:
    WEIGHT_DECAY: 0.0001
    LR_POLICY: steps_with_decay
    BASE_LR: 0.02
    GAMMA: 0.1
    MAX_ITER: 90000
    STEPS: [0, 60000, 80000]
FPN:
    FPN_ON: True
    MULTILEVEL_ROIS: True
    MULTILEVEL_RPN: True
FAST_RCNN:
    ROI_BOX_HEAD: fast_rcnn_heads.roi_2mlp_head
    ROI_XFORM_METHOD: RoIAlign
    ROI_XFORM_RESOLUTION: 7
    ROI_XFORM_SAMPLING_RATIO: 2
MRCNN:
    ROI_MASK_HEAD: mask_rcnn_heads.mask_rcnn_fcn_head_v1up4convs
    RESOLUTION: 28  # (output mask resolution) default 14
    ROI_XFORM_METHOD: RoIAlign
    ROI_XFORM_RESOLUTION: 14  # default 7
    ROI_XFORM_SAMPLING_RATIO: 2  # default 0
    DILATION: 1  # default 2
    CONV_INIT: MSRAFill  # default GaussianFill
TRAIN:
    SCALES: [800, ]
    MAX_SIZE: 1333
    BATCH_SIZE_PER_IM: 512
    RPN_PRE_NMS_TOP_N: 2000  # Per FPN level
TEST:
    SCALE: 800
    MAX_SIZE: 1333
    NMS: 0.5
    RPN_PRE_NMS_TOP_N: 1000  # Per FPN level
    RPN_POST_NMS_TOP_N: 1000
    
# ---------------------------------------------------------------------------- #
# Solver options
# Note: all solver options are used exactly as specified; the implication is
# that if you switch from training on 1 GPU to N GPUs, you MUST adjust the
# solver configuration accordingly. We suggest using gradual warmup and the
# linear learning rate scaling rule as described in
# "Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour" Goyal et al.
# https://arxiv.org/abs/1706.02677
# ---------------------------------------------------------------------------- #
SOLVER:
    # e.g 'SGD', 'Adam'
    TYPE: 'SGD'
    # Base learning rate for the specified schedule
    BASE_LR: 0.001
    # Schedule type (see functions in utils.lr_policy for options)
    # E.g., 'step', 'steps_with_decay', ...
    LR_POLICY: 'step'
    # Some LR Policies (by example):
    # 'step'
    #   lr = SOLVER.BASE_LR * SOLVER.GAMMA ** (cur_iter // SOLVER.STEP_SIZE)
    # 'steps_with_decay'
    #   SOLVER.STEPS = [0, 60000, 80000]
    #   SOLVER.GAMMA = 0.1
    #   lr = SOLVER.BASE_LR * SOLVER.GAMMA ** current_step
    #   iters [0, 59999] are in current_step = 0, iters [60000, 79999] are in
    #   current_step = 1, and so on
    # 'steps_with_lrs'
    #   SOLVER.STEPS = [0, 60000, 80000]
    #   SOLVER.LRS = [0.02, 0.002, 0.0002]
    #   lr = LRS[current_step]
    
    # Hyperparameter used by the specified policy
    # For 'step', the current LR is multiplied by SOLVER.GAMMA at each step
    GAMMA: 0.1
    # Uniform step size for 'steps' policy
    STEP_SIZE: 30000
    # Non-uniform step iterations for 'steps_with_decay' or 'steps_with_lrs'
    # policies
    STEPS: []
    # Learning rates to use with 'steps_with_lrs' policy
    LRS: []
    # Maximum number of SGD iterations
    MAX_ITER: 40000
    # Momentum to use with SGD
    MOMENTUM: 0.9
    # L2 regularization hyperparameter
    WEIGHT_DECAY: 0.0005
    # L2 regularization hyperparameter for GroupNorm's parameters
    WEIGHT_DECAY_GN: 0.0
    # Whether to double the learning rate for bias
    BIAS_DOUBLE_LR: True
    # Whether to have weight decay on bias as well
    BIAS_WEIGHT_DECAY: False
    # Warm up to SOLVER.BASE_LR over this number of SGD iterations
    WARM_UP_ITERS: 500
    # Start the warm up from SOLVER.BASE_LR * SOLVER.WARM_UP_FACTOR
    WARM_UP_FACTOR: 1.0 / 3.0
    # WARM_UP_METHOD can be either 'constant' or 'linear' (i.e., gradual)
    WARM_UP_METHOD: 'linear'
    # Scale the momentum update history by new_lr / old_lr when updating the
    # learning rate (this is correct given MomentumSGDUpdateOp)
    SCALE_MOMENTUM: True
    # Only apply the correction if the relative LR change exceeds this threshold
    # (prevents ever change in linear warm up from scaling the momentum by a tiny
    # amount; momentum scaling is only important if the LR change is large)
    SCALE_MOMENTUM_THRESHOLD: 1.1
    # Suppress logging of changes to LR unless the relative change exceeds this
    # threshold (prevents linear warm up from spamming the training log)
    LOG_LR_CHANGE_THRESHOLD: 1.1



  
